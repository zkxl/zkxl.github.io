---
layout: post
title: DataStructure - Array
date: 2017-04-12 13:00:00 -0700
categories: Study-Notes
tag: dataStructure
---
* content
{:toc}




Last Modified: 201704012


## Array

### Fixed size array

Array has its primitive implementation in C language:   
1. An array of size S is represented as a block of __S * size(int) bytes__ memory.
2. It doesn't have a __size__ pointer and therefore out-of-bound access causes __segmentation fault__.
3. __Indexing__ access A[i] translates into memory address access at (start of A) + i * size(int).
4. __Removing__ an element by memory leads to copy over all other elements after the removed one, one step ahead, leaving the array size unchanged.


__Note:__
1. Practically, it is surprisingly fast because of __good memory locality__.  
2. Another fast strategy is __lazy removal__, it simply marks the removed element as unaccessible. Indexing it no longer makes sense.

__Pros and Cons:__
1. Look up operation takes O(1) while it takes O(n) in a LinkedList.
2. Insertion takes O(n) while it takes O(1) in a LinkedList.(Not including finding the element.)


### Dynamic array

High-level description: [Dynamic array](https://en.wikipedia.org/wiki/Dynamic_array) wraps around primitive array.  


``` python
class DynamicArray():
  def __init__(self):
    memory_block self.A = malloc[initial_size]
    int self.size = initial_size # size pointer
    int self.length = 0 # actual length pointer
    # data struc invariant : length <= size

  def get(int i): # indexing access at i
    if i >= self.length: raise Exception("Out of bound.") # check index within bound
    return self.A[i] # translates to memory address access

  def put(int i, int val):
    if i >= self.length: raise Exception("Out of bound.") # check index within bound
    self.A[i] = val

  def append(int val):
    if self.length == self.size:
      memory_block B = malloc[constant * self.size]
      for i in range(self.length): # copy over
        B[i] = self.A[i]
      self.A = B
      self.size = constant * self.size
      # canonical expansion: size = 2 * size (potentially 50% memory waste)
      # expansion in Java: size = 1.5 * size
      # constant in Python: size = size + size >>> 3

    self.A[self.length] = val # append
    self.length += 1

  def pop():
    if self.length <= 0: raise Exception("Nothing to pop from empty array.")
    return self.A[self.length]

  def remove(int i):
    for j in range(i + 1, self.length+1):
      self.A[j-1] = copy(self.A[j])
    self.length -= 1
    # A few things discussed below.
    # ------------------------------------
    # in case of constant being 2, the following
    # "3/2 implementation" can give a good amortized time.
    # ------------------------------------
    if 3 * length == self.size:
      memory_block B = malloc[self.size/2]
      " copy over "
    self.A = B
    self.size = self.size/2
    return ;
```

* See amortized analysis for "3/2 implementation" in Mathematical Framework for Amortized Analysis.  

Removing an element from an array:  
1. takes O(n) if you create a new array and copy everything over except the target.  
2. takes O(n) if you move everything after the target upfront by 1 index.
3. takes O(1) if you don't care about its sequence, you can swap the target with the last element and do a pop. ? (pop? on array?)
4. takes O(1) if you'd rather mark the target as "unaccessible" than actually freeing up the memory.


__Amortized Analysis:__

If we can prove, any sequences of N operations on this data structure is in O(N), we can say that such an operation takes amortized O(1) time.  

The most expansive so far operation in dynamic array is "append" because it potentially takes O(N). Assuming the last append triggers array copy and thus takes O(N), in total: $$ \log_{2}^{N} $$ times of copy takes $$ O(N + n/2 + n/4 + ... 2 + 1) $$. N times of "direct" append takes O(N). Total amortized time is still O(N).


__Note:__
A trade off between potential space waste and ops efficiency. If the expansion factor shrinks from 2 (potential 50% waste) to 1.5, there will be higher constant factor placed on operation amortized time taken.

---

### __Mathematical Framework for Amortized Analysis:__
_(more example needed to make it clearer)_

__Define:__ [Potential Function](https://en.wikipedia.org/wiki/Potential_method)  $$ \Phi $$
1. map states of data structure to numbers.
2. must be 0 upon the initialization of the data structure.
3. no smaller than 0 after initialization.



__Define: Amortized Time__ = actual time + $$ C * (\Phi_{i} - \Phi_{j}) $$

1. _actual time: each operation's actual time cost._
2. _C: constant, arbitrarily large to make the amortized time ideal._
3. $$ \Phi $$ _: potential function._

With all the above constraints, we can say: __No matter how we choose potential function and C, we have: total time <= the sum of all operations' amortized time__ = time(ops1) + C($$ \Phi_1 - \Phi_0 $$) + time(ops2) + C($$ \Phi_2 - \Phi_1 $$) + ... + time(opsn) + C($$ \Phi_n - \Phi_{n-1} $$).

__In the case of dynamic array: Potential function = max(0, 2 * length - size)__  

|                    | actual    | change        | total
|--------------------|-----------|---------------|------
| remove             | O(1)      | -2            | O(1)
| add_non-reallocate | O(1)      | +2            | O(1)
| add_reallocate     | O(length) | 2 - length    | O(1)
| "3/2" removal      | O(length) | 2 + 3/2 length| O(1)

* __add_reallocate:__ 2 - length = ((2 * length - new size before operation) - (2 * length - old size after operation) + 2) = (0 - length) + 2. This operation is only performed when length == size.
* __"3/2" removal:__ 2 - length = ((2 * length - new size before operation) - (2 * length - old size after operation) + 2) = (1/2 * length - (- length)) + 2

In the above case, as long as we choose C greater than the constant in O(length), we can have O(1) total time.  

If we define: $$ \Phi = 0 $$ means the data structure is in an ideal state. $$ \Phi $$ now has concrete meaning instead of an abstract mathematical measure - the distance from ideal state.
