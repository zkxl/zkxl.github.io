---
layout: post
title: convNets
date: 2017-01-05 18:06:00 -0800
categories: deeplearning-basics
tag: convNets
---

* content
{:toc}



__These deeplearning series are inspired by Stanford open course â€” cs231n. My special thanks.__


_Deeplearning is involved in a wide range of techniques that allow a neural network to make reliable predications. This series will introduce several key components of a neural network, from linear classifier loss functions to fully connected neural nets, from convNets to RNN._


---

### Helper functions of convNets

Helper functions:
* pad image with 0 for some pixels;
* crop images;
* make square images into a bar of pixels;
* restore the image from a bar.

---

paddedX = images_padded_with(x, pad)

``` python
def images_padded_with(x, pad):

  """
  Input:
  - x: Input image data of shape (N, C, H, W)
  - pad: number of pixels to be paded
  Output:
  - padded image data
  """

  return paddedX;
```

---

croppedX = def images_cropped_with(paddedX, pad)

``` python
def images_cropped_with(paddedX, pad):

  """
  This fnc undo what the images_padded_with() fnc does

  Input:
  - paddedX: Input image data of shape (N, C, H, W)
  - pad: pixels to be cropped
  Output:
  - out: cropped image data
  """

  return croppedX
```

---

imgbars = imgs2col_matrice(x, filter_size, stride, pooling = False)

``` python
def imgs2col_matrice(x, filter_size, stride, pooling = False):

  """
  Input:
  - x: Input data of shape (N, C, H, W)
  - window_size: a tuple consisting of (window_height, window_width) - essentially like HH, WW in w.shape
  - stride: pace
  - pooling: the fnc was originally designed for conv layer, now got modified a bit so it can be used in pooling layer as well

  Return:
  - out: x being reshaped into a 2D matrice - (number_local_windows, window_size)

  For a single (most likely padded) img(3, 227, 227)
    convolving with 96 filters of (3, 11, 11) at stride 4
    it forms output(96, 55, 55).
  Now we:
    1. cut off a chunk of img of the same size with filter at each stride, there will be 55 * 55 = 3025 such chunks.
       pull each chunk into a 1D array(1, 3 * 11 * 11) and lay each of them on top of another, img -> (3025, 3 * 11 * 11)
    2. reshape filters w into (96, 3 * 11 * 11), each filter became a 1D array(1, 3 * 11 * 11)
    3. apply np.dot(img, filters.T) -> output(3025, 96)

  This fnc implements the 1st of the above operations for N imgs.
  """

  return out

```

---

x = col2imgs_matrice(x_unfolded, x_shape, filter_size, stride, accumulative = True, pooling = False)

``` python
def col2imgs_matrice(x_unfolded, x_shape, filter_size, stride, accumulative = True, pooling = False):

  """
  this fnc implements the reverse transformation of what imgs2col_matrice() does

  Input:
  - x_unfolded: the output of imgs2col_matrice(), shape: (N * OH * OW, C * HH * WW)
  - x_shape: specify the output shape
  - window_size: specify the window scale
  - stride: pace
  - accumulative: If it is called to restore the image, accumulative = False; If it is called to accumulate gradients dx, accumulative = True.
  - pooling: bool default is False, again, this fnc helps to reconstruct the pooling gradients into the shape of the input x when they flow back

  Output:
  - x: image data of shape (N, C, H, W)
  """

  return x;
```

---

### Convolution layer

* Forward mapping and backward propagation in convNets are essentially the same thing as those in fully-connected neural nets.
* But in convNets, instead of sequentially assign weights to each pixel, it will slide a, typically 3 x 3 x H, 3D receptive field to extract spatial info. (what's above/left/right/below? What's the color? ... etc)
* Compared with a 7 x 7 x H receptive field, smaller receptive field tend to give higher nonlinearity and less parameters to update, which are advantages.

``` python
def conv_forward_efficient(x, w, b, conv_param):

  """
  It implements the conv layer forward pass based on efficient matrices dot product operation.

  The above imgs2col_matrice() implemented the 1st step
  Now we complete the following
    2. reshape filters w into (96, 3 * 11 * 11), each filter became a 1D array(1, 3 * 11 * 11)
    3. apply np.dot(img, filters.T) -> output(3025, 96) -> reshape into (96, 55, 55)

  The input consists of N data points, each with C channels, height H and width
  W. We convolve each input with F different filters, where each filter spans
  all C channels and has height HH and width HH.

  Input:
  - x: Input data of shape (N, C, H, W)
  - w: Filter weights of shape (F, C, HH, WW)
  - b: Biases, of shape (F,)
  - conv_param: A dictionary with the following keys:
    - 'stride': The number of pixels between adjacent receptive fields in the
      horizontal and vertical directions.
    - 'pad': The number of pixels that will be used to zero-pad the input.

  Returns a tuple of:
  - out: Output data, of shape (N, F, H', W') where H' and W' are given by
    H' = 1 + (H + 2 * pad - HH) / stride
    W' = 1 + (W + 2 * pad - WW) / stride
  - cache: (x, w, b, conv_param)
  """

  out = None
  stride, pad = conv_param["stride"], conv_param["pad"]

  F, C, HH, WW = w.shape
  N, _, H, W = x.shape
  OH = 1 + (H + 2 * pad - HH) / stride
  OW = 1 + (W + 2 * pad - WW) / stride

  paddedX = images_padded_with(x, pad) # x(N, C, H, W) -> paddedX(N, C, H + 2pad, W + 2pad)

  imgbars = imgs2col_matrice(paddedX, (C, HH, WW), stride)
  # completely comb it into a vertical line of imgbars(N * OH * OW, C * HH * WW) -> (number_local_windows, window_size)

  w_strips = w.reshape(F, C * HH * WW).T # transpose it (filter_size, filter_numbers) ready for dot product.

  product = np.dot(imgbars, w_strips) + b # (N * OH * OW, F) + (, F) -> (number_local_windows, number_filters)

  reshaped_product = product.T.reshape(F, N, OH, OW) # .T so that it can be correctly reshaped (((OH, OW) for N times) for F times)

  out = np.transpose(reshaped_product, axes = (1, 0, 2, 3)) # switch the first and the second axes to match out format

  cache = (x, w, b, conv_param, imgbars, w_strips)

  return out, cache


def conv_backward_efficient(dout, cache):
  """
  A naive implementation of the backward pass for a convolutional layer.

  Inputs:
  - dout: Upstream derivatives.
  - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive

  Returns a tuple of:
  - dx: Gradient with respect to x - (N, C, H, W)
  - dw: Gradient with respect to w - (F, C, HH, WW)
  - db: Gradient with respect to b - (F, )

  """

  x, w, b, conv_param, imgbars, w_strips = cache
  stride, pad = conv_param["stride"], conv_param["pad"]

  # imgbars: (N * OH * OW, C * HH * WW)
  # w_strips: (C * HH * WW, F)
  # product: (N * OH * OW, F)

  N, _, H, W = x.shape
  F, C, HH, WW = w.shape  

  # the following is the reverse transformation of "product -> out" in forward pass
  # this transformation is critical and becomes the basis for all the matrices computation that follow.
  dout = np.transpose(dout, axes = (1, 0, 2, 3))
  dout = dout.reshape(F, -1)
  # gate: out = x * w + b; So: dw = x * dout
  dw = np.dot(dout, imgbars)
  dw = dw.reshape(F, C, HH, WW)

  # gate: out = x * w + b; So: dx = w * dout
  dx_bars = np.dot(w_strips, dout).T # .T so that dx_bars.shape matches imgbars_like.shape to be correctly processed by col2imgs_matrice() as follows
  # the following is the reverse transformation of what the imgs2col_matrice() does in forward pass
  padded_x_shape = (N, C, H + 2 * pad, W + 2 * pad)
  dx = col2imgs_matrice(dx_bars, padded_x_shape, (C, HH, WW), stride, accumulative = True)
  dx = images_cropped_with(dx, pad)

  # gate: out = x * w + b; So: db = np.sum(dout, axis = 1)
  db = np.sum(dout, axis = 1)

  return dx, dw, db
```
---

### Max-pooling

* Max-pooling performs a down-sampling operation

``` python

def max_pool_forward_efficient(x, pool_param):

  """
  A naive implementation of the forward pass for a max pooling layer.

  Inputs:
  - x: Input data, of shape (N, C, H, W)
  - pool_param: dictionary with the following keys:
    - 'pool_height': The height of each pooling region
    - 'pool_width': The width of each pooling region
    - 'stride': The distance between adjacent pooling regions

  Returns a tuple of:
  - out: Output data
  - cache: (x, pool_param)
  """

  out = None
  PH, PW, stride = pool_param['pool_height'], pool_param['pool_width'], pool_param['stride']

  imgbars = imgs2col_matrice(x, (1, PH, PW), stride, pooling = True)
  # def imgs2col_matrice(x, filter_size, stride, pooling = False): return out;

  maxbars = np.amax(imgbars, axis = 1, keepdims = True) # 2D

  N, D, H, W = x.shape
  OH = (H - PH) / stride + 1
  OW = (W - PW) / stride + 1
  # need to figure out the dimensions so that I can reshape maxbars into output
  out = maxbars.reshape(N, D, OH, OW)

  cache = (x.shape, imgbars, maxbars, pool_param)

  return out, cache


def max_pool_backward_efficient(dout, cache):

  """
  A naive implementation of the backward pass for a max pooling layer.

  Inputs:
  - dout: Upstream derivatives
  - cache: A tuple of (x, pool_param) as in the forward pass.

  Returns:
  - dx: Gradient with respect to x
  """

  x_shape_recovered, imgbars, maxbars, pool_param = cache
  PH, PW, stride = pool_param['pool_height'], pool_param['pool_width'], pool_param['stride']

  # gate: out_pixel = max(x_window) -> pass dout through where imgbars == maxbars is True (indices of the max value chosen) and 0 through other indices else

  # expand dout -> into doutbars of the same shape as imgbars, so the correct value can be calved out of doutbars when "imgbars == maxbar" works as a mask
  doutbars = np.dot(dout.reshape(-1, 1), np.ones((1, imgbars.shape[1])))

  # np.where(condition(bool table), value_table where True, value_table where False) # these three tables have the same shape
  dx = np.where(imgbars == maxbars, doutbars, np.zeros_like(doutbars))

  # def col2imgs_matrice(x_unfolded, x_shape, filter_size, stride, accumulative = True, pooling = False): return x;
  dx = col2imgs_matrice(dx, x_shape_recovered, (1, PH, PW), stride, pooling = True)

  return dx
```

---

### ConvNets batch normalization

* This is based on the vanilla batch normalization implemented in fully-connected-nn post.

``` python
def spatial_batchnorm_forward(x, gamma, beta, bn_param):

  """
  Normally batch-normalization accepts inputs of shape (N, D) and produces outputs of shape (N, D), where we normalize across the minibatch dimension N.
    For data coming from convolutional layers, batch normalization needs to accept inputs of shape (N, C, H, W) and produce outputs of shape (N, C, H, W)
    where the N dimension gives the minibatch size and the (H, W) dimensions give the spatial size of the feature map.

  If the feature map was produced using convolutions, then we expect the statistics of each feature channel to be relatively consistent both between different images and different locations within the same image.
    Therefore spatial batch normalization computes a mean and variance for each of the C feature channels by computing statistics over both the minibatch dimension N and the spatial dimensions H and W.

  Computes the forward pass for spatial batch normalization.

  Inputs:
  - x: Input data of shape (N, C, H, W)
  - gamma: Scale parameter, of shape (C,)
  - beta: Shift parameter, of shape (C,)
  - bn_param: Dictionary with the following keys:
    - mode: 'train' or 'test'; required
    - eps: Constant for numeric stability
    - momentum: Constant for running mean / variance. momentum=0 means that
      old information is discarded completely at every time step, while
      momentum=1 means that new information is never incorporated. The
      default of momentum=0.9 should work well in most situations.
    - running_mean: Array of shape (D,) giving running mean of features
    - running_var Array of shape (D,) giving running variance of features

  Returns a tuple of:
  - out: Output data, of shape (N, C, H, W)
  - cache: Values needed for the backward pass
  """

  out, cache = None, None

  N, C, H, W = x.shape

  # imagine you are looking at the color channels of an image (C, H, W), then you take each channel as a feature and roll it out into (-1, C)
  # now the following does that on a mini-batch of N images.
  rollout = np.transpose(x, (0, 2, 3, 1)).reshape(-1, C)

  # def batchnorm_forward(x, gamma, beta, bn_param): return out, cache;
  out, cache = batchnorm_forward(rollout, gamma, beta, bn_param)

  # restore the shape of the output
  out = np.transpose(out.reshape(N, H, W, C), (0, 3, 1, 2))

  return out, cache


def spatial_batchnorm_backward(dout, cache):
  """
  Computes the backward pass for spatial batch normalization.

  Inputs:
  - dout: Upstream derivatives, of shape (N, C, H, W)
  - cache: Values from the forward pass

  Returns a tuple of:
  - dx: Gradient with respect to inputs, of shape (N, C, H, W)
  - dgamma: Gradient with respect to scale parameter, of shape (C,)
  - dbeta: Gradient with respect to shift parameter, of shape (C,)
  """

  dx, dgamma, dbeta = None, None, None

  N, C, H, W = dout.shape

  rollout = np.transpose(dout, (0, 2, 3, 1)).reshape(-1, C)

  # def batchnorm_backward(dout, cache): return dx, dgamma, dbeta
  dx, dgamma, dbeta = batchnorm_backward(rollout, cache)

  dx = np.transpose(dx.reshape(N, H, W, C), (0, 3, 1, 2))

  return dx, dgamma, dbeta
```

---

###NOTE

with all the small building blocks defined above, of course you can stitch conv_layer - spatial_batchnorm - ReLU together to create a convenient layer as a bigger building block for your model. The design of a convNets architecture will be more challenging than a fully-connected architecture due to the difficulties to keep track of the input-output dimensions of each layer.
