---
layout: post
title: DataStructure - HashMap
date: 2017-04-12 13:00:00 -0700
categories: Study-Notes
tag: dataStructure
---
* content
{:toc}




## Dictionary / HashMap

Last Modified: 201704012

Let's see how it came from the most primitive abstract to the most useful data structure in modern languages.  

[Dictionary](https://en.wikipedia.org/wiki/Associative_array) is the __abstract idea__ on a collection of (k, v) pairs with one key mapping to one value.  

Essential Operations:
1. Look-up (static/dynamic dictionary).
2. Store new value [add or replace] (dynamic dictionary).
3. Remove (k, v) pair.

### Classic Use case - Decorator Pattern

__Description:__ For some graph algorithms, where you want to associate several fields of info with every vertex without modifying the vertex.

__Solution:__
1. Each vertex is associated with a dictionary, where (field_name, info) are stored.
2. Each field is associated with a dictionary, where (vertex, info) are stored. _Things will not screw up if field_name changes._  

### Implementation - HashMap

__Maintain:__
1. An array of size N, greater than the number of (k, v) pairs. So __load factor__ < 1.
2. A hash function.

__Hash Function:__

Regardless of the choice of hash function, the following assumption is held as standard to conclude that __dict ops are O(1): Hash function's mapping of key to index is uniformly distributed and independent of other keys__.

__Performance Analysis:__  

With:
1. An array of size N
2. A uniformly distributed hash function
3. n entries in total  


Collision occurs while looking up one of them with $$ {\frac{n-1}{N}} $$ probability. Each collision takes one comparison. So the look-up operation takes time of (load_factor - 1/N) = O(1).

The probability of one key collide with a specific key is (1/N). Each collision costs one comparison.

__Collision__ is unavoidable because [birthday paradox.](https://en.wikipedia.org/wiki/Birthday_problem)

### Different Hashing to Counter Collision

#### Hash chaining  

GET: Find bucket and match key.  
SET: Find bucket and append new key value pair.  
REMOVE: Find bucket and do the same as remove from an array.  
Lazy Remove: Find bucket, match key and mark as "unaccessible".  

``` python
# initialization
Array<List<Pair>> H = new Array<List<Pair>> # associative list

# SET(key, value) Method - find bucket and append
for pairs in H[hash(key)]:
  if pair.key == key:
    pair.value = value
    return ;
H[hash(key)].append(new Pair(key, value))
return ;

# GET(key) Method
for pair in H[hash(key)]:
  if pair.key == key: return pair.value;
raise Exception("can't find the key.")

# Note: see why hash chaining is object-heavy? These Pairs() obj can be replaced by Tuple() and no more reduction.

# REMOVE(key)
bucket = H[hash(key)]
for i in range(len(bucket)):
  pair = bucket[i]
  if pair.key == key: # find the removable
    for j in range(i + 1, len(bucket)): # toggle everything after
      bucket[j - 1] = bucket[j] # decrement length as needed
    return

# Lazy Removal(key) requires modifications on GET and SET.
for i in range(len(bucket)):
  pair = bucket[i]
  if pair.key == key:
    pair.key = "unaccessible" # mark as unaccessible
  else: # if pair.key is a mismatch or unaccessible
    continue # skip it without decrementing length
```

#### Open addressing - linear probing  

GET: Find bucket and probe the next one to match key.  
SET: Find bucket and probe the next empty one.  
__REMOVE__: Roll back one at a time.
Lazy Remove: Mark "unaccessible". (Create table pollution and drag down the data structure performance as the number of deletions goes high)

``` python
# initialization
Array<Pair> H = new Array<Pair>
int N = H.size
int n = " the number of current entries in H "
int load_factor = 0.5

# SET(key, value) Method
bucket = hash(key)
while True:
  pair = H[bucket]
  if pair.key = key: # reset value for this key
    pair.value = value
    return ;
  elif pair.isEmpty():
    if (n / N) > 0.5:
      # check to ensure the efficiency of this data structure.
      " reallocate bigger memory to H and copy over existing entries. "
    pair.key = key
    pair.value = value
    n += 1
    return ;
  else: # the bucket has been taken
    bucket = (bucket + 1) % N

# GET(key) Method
bucket = hash(key)
while True:
  pair = H[bucket]
  if pair.key == key:
    return pair.value
  elif pair.isEmpty():
    raise Exception("KeyNotFound Error")
  else:
    bucket = (bucket + 1) % N

# REMOVE(key)
bucket = hash(key)
while True:
  if (bucket + 1) % N <= bucket: # turn around
    if H[(bucket + 1) % N].key == key:
      H[bucket] = H[(bucket + 1) % N] # replace
    bucket = (bucket + 1) % N
    continue
  if H[bucket + 1].isEmpty():
    " empty H[bucket] "
    return;
  else:
    if H[bucket + 1].key == key:
      H[bucket] = H[bucket + 1]
    bucket += 1
```

#### Chaining Vs. Linear Probing  

1. In __theory__, chaining and probing give O(1) performance when load_factor is a lot smaller than 1. However when load_factor exceeds 1, chaining still works with a slightly higher constant. But the performance of probing will be destroyed.
2. In __practice__, probing is efficient because chaining involves going from hash cell to dynamic array which is an object wrapping around array. On the other hand, probing has better __memory locality__. Random memory access is expensive compared with cached-in memory operations.
3. Linear probing requires a higher-quality hash function. In case of low-quality hash function, a big __continuous chunk of array__ will be occupied, which drags down the performance significantly.

__Linear Probing Requires High-Quality Hash Function.__ Here's why:  
If the hash function can't produce uniformly distributed results. It's possible that we have a continuous chunk of array occupied. Let's say such block i has length $$ L_{i} $$ and every operation cost is $$ O(L_{i}) $$.   

(?? some thing wrong here) The cost of n such operations is:  

$$ O({\frac{1}{n}} \sum_{block_i} L_{i}^{2}) $$

Now with load_factor = $$ \alpha $$, the expected number of keys mapped to any of such blocks is: $$ \alpha L $$. According to [Churnoff Bound](https://en.wikipedia.org/wiki/Chernoff_bound), the probability (P) of mapping a key to a continuous block:  

$$ P <= ({\frac{e^{\epsilon}}{(1+\epsilon)^{1+\epsilon}}})^{\alpha L} $$

s.t:  

$$ (1 + \epsilon) \alpha = 1 $$

In case $$ \alpha = 0.5 $$, $$ P < {\frac{e}{4}}^{\frac{L}{2}} $$

### Cuckoo Hashing

Pros:
+ constant worse-case time for lookup and delete ops.  

Cons:
+ With a load_factor < 1/2, insertion fails at probability of (1 / N^2) where N is the size of the hash table. When fail occurs, table needs to be rebuilt.  
+ Bigger load_factor causes drastic increase of insertion failures.  
+ Thus, deterministic hash functions cannot be used. Otherwise, rebuild makes no sense.  

``` python
# Initialize two hash table with size N/2 for each
Array<Pair> H0 = new Array<Pair> with hash function h0
Array<Pair> H1 = new Array<Pair> with hash function h1

# SET(key, value)
itern = 0
while True:
  if H0[h0(key)].isEmpty():
    H0[h0(key)] = new Pair(key, value)
    return
  if H1[h1(key)].isEmpty():
    H1[h1(key)] = new Pair(key, value)
    return
  if H0[h0(key)].key == key:
    H0[h0(key)].key = key
    H0[h0(key)].value = value
    return
  if H1[h1(key)].key == key:
    H1[h1(key)].key = key
    H1[h1(key)].value = value
    return
  # if both of them are not empty but none contains the key
  # start kicking from 0
  tmp0 = H0[h0(key)]
  H0[h0(key)] = new Pair(key, value)
  tmp1 = H1[h1(key)]
  H1[h1(key)] = tmp0
  key = tmp1.key
  value = tmp1.value
  if itern > threshold: start rebuilding
  itern += 1


# GET(key)
if H0[h0(key)].key == key:
  return H0[h0(key)].value
if H1[h1(key)].key == key:
  return H1[h1(key)].value
raise Exception("Not found.")

# REMOVE(key)
if H0[h0(key)].key == key:
  "clear the cell"
  return
if H1[h1(key)].key == key:
  "clear the cell"
  return
raise Exception("Not found.")
```

#### Graph Representation of Cuckoo Hashing

Graph: G = (V, E)  
Vertices: V = table cells of H0 and H1  
Edges: E = pairs of cells connected by hashing the same key through h0 and h1  

1. Direct edges to point to where each key is stored.
2. A valid assignment is: each vertex has <= 1 incoming edge.
3. Insertion could reverse edges along a single path to try out different possible assignment.

__If and only if all the components of G is a tree or a tree + one edge (potentially one circle allowed). Otherwise, infinite loop.__

When load_factor <= 1/2, there are $$ \Theta(N) $$ small trees and $$ \Theta(1) $$ tree + one edge. When load_factor gets even slightly larger than 1/2, small trees will gradually turn into a giant component, where infinite loops arise.

__Note:__ If you want it to work without half space wasted, a trick you can do is let every array cell stores a constant number of (k, v) pairs, which differs from hash chaining in that only constant number of pairs are allowed so it is still constant time access in a sense.  


### Hash Function

1. Cryptographic hashing functions, such as SHA-256 and MD5, are a good candidate in theory. But they are too slow to be used in a data structure.
2. [Tabulation hasing.](https://en.wikipedia.org/wiki/Tabulation_hashing#Method)
3. Polynomial random linear hashing.

_Side note: MD5 stands for message digest 5._

#### Tabulation Hashing

Quote from Wiki -  
_"Let p denote the number of bits in a key to be hashed, and q denote the number of bits desired in an output hash function. Choose another number r, less than or equal to p; this choice is arbitrary, and controls the tradeoff between time and memory usage of the hashing method: smaller values of r use less memory but cause the hash function to be slower. Compute t by rounding p/r up to the next larger integer; this gives the number of r-bit blocks needed to represent a key. For instance, if r = 8, then an r-bit number is a byte, and t is the number of bytes per key. The key idea of tabulation hashing is to view a key as a vector of t r-bit numbers, use a lookup table filled with random values to compute a hash value for each of the r-bit numbers representing a given key, and combine these values with the bitwise binary exclusive or operation. The choice of r should be made in such a way that this table is not too large; e.g., so that it fits into the computer's cache memory."_ [[1]](https://books.google.com/books?id=vMqSAwAAQBAJ&pg=SA11-PA3#v=onepage&q&f=false)

__Here is an example of how it works:__  
The hash function h(x) maps 32-bit keys to 16-bit hash values by breaking each key into four 8-bit bytes, using each byte as the index into four tables of 16-bit random numbers (each table has $$ 2^{8} $$ such random numbers), and returning the bitwise exclusive or of the four numbers found in this table.

#### Polynomial random linear hashing

For a hash table with size N,  

Set a large prime number P (P >> N) so that hash(x) = (x % P) % N is close to being uniformly distributed.  

Set a small positive integer d as polynomial degree and generate a set of random coefficients $$\alpha_i $$ (each coefficient add a degree of "uniformality") so that:  

$$ hash(x) = ((\sum_{i=0}^{d} x_i \alpha_i)\;mod\;P)\;mod\;N $$

where, if two keys: k1 and k2 have the same digest, $$ \sum_{i=0}^{d} x_i \alpha_i $$ is still uniformly distributed.  

which means:  

$$ Pr(collision) = 1/N $$

Note: P is either a random large number or a fixed number larger than the maximum key to ensure the above properties.  

__Introduce K-independent Hashing:__

The concept was derived from the above polynomial random linear hashing function. Such a function with degree d is viewed as d-independent hashing function.  

Formally defined as: K-independent hash function is a family of hash functions that can map any set of K keys to a set of K independent random variables.

Quote from Wiki -  
_"The original technique for constructing k-independent hash functions, given by Carter and Wegman, was to select a large prime number p, choose k random numbers modulo p, and use these numbers as the coefficients of a polynomial of degree k whose values modulo p are used as the value of the hash function. All polynomials of the given degree modulo p are equally likely, and any polynomial is uniquely determined by any k-tuple of argument-value pairs with distinct arguments, from which it follows that any k-tuple of distinct arguments is equally likely to be mapped to any k-tuple of hash values."_ [[2]](http://www.fi.muni.cz/~xbouda1/teaching/2009/IV111/Wegman_Carter_1981_New_hash_functions.pdf)
